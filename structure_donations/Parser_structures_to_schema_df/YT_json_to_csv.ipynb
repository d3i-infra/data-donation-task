{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/json_structure_donations/processed_structure_donations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object']\n",
    "columns = ['col_path_1','col_path_2','col_path_3','col_path_4', 'col_path_5']\n",
    "\n",
    "\n",
    "def process_row_path(row, columns, data_types):\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "        if row[column] in data_types:\n",
    "            row['data_type'] = row[column]\n",
    "            row[column] = np.nan\n",
    "        else:\n",
    "            row[column]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "    # Create different colums for each part of the document path\n",
    "    df['path_1'] = df['variable'].str.split('/', n=1).str[0]\n",
    "    df['path_2'] = df['variable'].str.split('/', n=3).str[1]\n",
    "    df['path_3'] = df['variable'].str.split('/', n=3).str[2]\n",
    "    df['path_4'] = df['variable'].str.split('/', n=3).str[3]\n",
    "\n",
    "    # Create a column with the JSON name\n",
    "    df['json_name'] = df['variable'].str.rsplit('/', n=1).str[-1]\n",
    "\n",
    "\n",
    "    # As the JSON name is stored in the json_name column, fill other parts of the path with Na if the name of the JSON is present\n",
    "    mark = \".json\"\n",
    "\n",
    "    df['path_2'] = df['path_2'].apply(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "    df['path_3'] = df['path_3'].apply(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "    df['path_4'] = df['path_4'].apply(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "\n",
    "    # Unlist the value column (where the JSON info is stored)\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['value'], list):\n",
    "            df.at[index, 'value'] = row['value'][0]\n",
    "    \n",
    "    print('file_paths')\n",
    "    display(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_column_paths(df):\n",
    "    # Initialize the new columns\n",
    "    df['row_path'] = '' \n",
    "    df['col_path_1'] = ''  \n",
    "    df['col_path_2'] = ''  \n",
    "    df['col_path_3'] = ''\n",
    "    df['col_path_4'] = ''\n",
    "    df['col_path_5'] = '' \n",
    "\n",
    "   \n",
    "    # Extract the level 1 keys \n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index, 'row_path'] = list(row['value'].keys())\n",
    "\n",
    "    print('value', df['value'])\n",
    "    print('row_path',df['row_path'])\n",
    "\n",
    "    #df['row_path'] = df['row_path'].astype('str').apply(eval).str[0]\n",
    "    print('row_path',df['row_path'])\n",
    "\n",
    "\n",
    "    \n",
    "    # Take the level 1 keys stored in a list and store them in individual rows\n",
    "    df = df.explode('row_path')\n",
    "  \n",
    "\n",
    "    def column_paths(df, row_var, get_var, unlist_var, explode_var):\n",
    "        \n",
    "        print('row_var', df[row_var])\n",
    "        print('get_var', df[get_var])\n",
    "        \n",
    "        # Extract nested values using keys\n",
    "        df[unlist_var] = df.apply(lambda row: [row[row_var].get(row[get_var], None)] if isinstance (row[row_var], dict) else None, axis=1)\n",
    "\n",
    "        print('unlist var', df[unlist_var])\n",
    "\n",
    "        # Unlist and explode\n",
    "        df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "        df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        df[explode_var] = df[unlist_var]\n",
    "        df = df.explode(explode_var)\n",
    "\n",
    "    \n",
    "        \n",
    "        return df\n",
    "\n",
    "    row_var = ['value', 'col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values']\n",
    "    get_var = ['row_path', 'col_path_1', 'col_path_2', 'col_path_3', 'col_path_4']\n",
    "    unlist_var = ['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values']\n",
    "    explode_var = ['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5']\n",
    "\n",
    "    for r, g, u, e in zip(row_var, get_var, unlist_var, explode_var):\n",
    "        df = column_paths(df, r, g, u, e)\n",
    "        row_var = unlist_var\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    " \n",
    "    # Reorder the columns in the df\n",
    "    df = df.loc[:, ['variable', 'value', 'path_1', 'path_2',\n",
    "        'path_3', 'path_4', 'json_name', 'row_path', 'col_path_1','col_path_2','col_path_3', 'col_path_4', 'col_path_5','data_type', 'col_path_1_values','col_path_2_values','col_path_3_values', 'col_path_4_values', 'col_path_5_values']]\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.fillna('Missing')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    df.to_csv(f\"{main_path}Youtube/Output/Output_\" + file_name + '.csv', index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    data = Path(data)  \n",
    "    #file_name = data.name  # Extracts \"data.json\"\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "\n",
    "    # Flatten JSON (handling nested structures)\n",
    "    df = pd.json_normalize(data, max_level=0)\n",
    "\n",
    "    # Delete user specific informations\n",
    "    df.columns = df.columns.str.replace(r'^[^/]+/', '', regex=True)\n",
    "\n",
    "    # Extract column names\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "\n",
    "    # From wide to long df\n",
    "    df = pd.melt(df, value_vars= cols)\n",
    "\n",
    "    df = file_paths(df)\n",
    "    \n",
    "    df = row_column_paths(df)\n",
    "    \n",
    "    df = df.apply(lambda row: process_row_path(row, columns, data_types), axis=1)\n",
    "    \n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/json_structure_donations/processed_structure_donations/Youtube/Input\n"
     ]
    }
   ],
   "source": [
    "input_directory = Path(f'{main_path}Youtube/Input')  \n",
    "print(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/json_structure_donations/processed_structure_donations/Youtube/Input/json_structure_youtube.json\n",
      "file_paths\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>path_1</th>\n",
       "      <th>path_2</th>\n",
       "      <th>path_3</th>\n",
       "      <th>path_4</th>\n",
       "      <th>json_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube and YouTube Music/history/watch-histor...</td>\n",
       "      <td>{'header': 'string', 'title': 'string', 'title...</td>\n",
       "      <td>YouTube and YouTube Music</td>\n",
       "      <td>history</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watch-history.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            variable  \\\n",
       "0  YouTube and YouTube Music/history/watch-histor...   \n",
       "\n",
       "                                               value  \\\n",
       "0  {'header': 'string', 'title': 'string', 'title...   \n",
       "\n",
       "                      path_1   path_2  path_3  path_4           json_name  \n",
       "0  YouTube and YouTube Music  history     NaN     NaN  watch-history.json  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value 0    {'header': 'string', 'title': 'string', 'title...\n",
      "Name: value, dtype: object\n",
      "row_path 0    [header, title, titleUrl, subtitles, time, pro...\n",
      "Name: row_path, dtype: object\n",
      "row_path 0    [header, title, titleUrl, subtitles, time, pro...\n",
      "Name: row_path, dtype: object\n",
      "row_var 0    {'header': 'string', 'title': 'string', 'title...\n",
      "0    {'header': 'string', 'title': 'string', 'title...\n",
      "0    {'header': 'string', 'title': 'string', 'title...\n",
      "0    {'header': 'string', 'title': 'string', 'title...\n",
      "0    {'header': 'string', 'title': 'string', 'title...\n",
      "0    {'header': 'string', 'title': 'string', 'title...\n",
      "0    {'header': 'string', 'title': 'string', 'title...\n",
      "Name: value, dtype: object\n",
      "get_var 0              header\n",
      "0               title\n",
      "0            titleUrl\n",
      "0           subtitles\n",
      "0                time\n",
      "0            products\n",
      "0    activityControls\n",
      "Name: row_path, dtype: object\n",
      "unlist var 0                                   [string]\n",
      "0                                   [string]\n",
      "0                                   [string]\n",
      "0    [[{'name': 'string', 'url': 'string'}]]\n",
      "0                                   [string]\n",
      "0                                 [[string]]\n",
      "0                                 [[string]]\n",
      "Name: col_path_1_values, dtype: object\n",
      "row_var 0                                 string\n",
      "0                                 string\n",
      "0                                 string\n",
      "0    {'name': 'string', 'url': 'string'}\n",
      "0    {'name': 'string', 'url': 'string'}\n",
      "0                                 string\n",
      "0                                 string\n",
      "0                                 string\n",
      "Name: col_path_1_values, dtype: object\n",
      "get_var 0    string\n",
      "0    string\n",
      "0    string\n",
      "0      name\n",
      "0       url\n",
      "0    string\n",
      "0    string\n",
      "0    string\n",
      "Name: col_path_1, dtype: object\n",
      "unlist var 0        None\n",
      "0        None\n",
      "0        None\n",
      "0    [string]\n",
      "0    [string]\n",
      "0        None\n",
      "0        None\n",
      "0        None\n",
      "Name: col_path_2_values, dtype: object\n",
      "row_var 0      None\n",
      "0      None\n",
      "0      None\n",
      "0    string\n",
      "0    string\n",
      "0      None\n",
      "0      None\n",
      "0      None\n",
      "Name: col_path_2_values, dtype: object\n",
      "get_var 0      None\n",
      "0      None\n",
      "0      None\n",
      "0    string\n",
      "0    string\n",
      "0      None\n",
      "0      None\n",
      "0      None\n",
      "Name: col_path_2, dtype: object\n",
      "unlist var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_3_values, dtype: object\n",
      "row_var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_3_values, dtype: object\n",
      "get_var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_3, dtype: object\n",
      "unlist var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_4_values, dtype: object\n",
      "row_var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_4_values, dtype: object\n",
      "get_var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_4, dtype: object\n",
      "unlist var 0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "0    None\n",
      "Name: col_path_5_values, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        print(file)\n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable             0\n",
      "value                0\n",
      "path_1               0\n",
      "path_2               0\n",
      "path_3               8\n",
      "path_4               8\n",
      "json_name            0\n",
      "row_path             0\n",
      "col_path_1           6\n",
      "col_path_2           8\n",
      "col_path_3           8\n",
      "col_path_4           8\n",
      "col_path_5           8\n",
      "data_type            0\n",
      "col_path_1_values    0\n",
      "col_path_2_values    6\n",
      "col_path_3_values    8\n",
      "col_path_4_values    8\n",
      "col_path_5_values    8\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1545218/1871176792.py:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_final = df_final.replace('Missing', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Youtube/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "common_columns = ['variable', 'value', 'path_1', 'path_2',\n",
    "        'path_3', 'path_4', 'json_name', 'row_path', 'col_path_1','col_path_2','col_path_3', 'col_path_4', 'col_path_5','data_type', 'col_path_1_values','col_path_2_values','col_path_3_values', 'col_path_4_values', 'col_path_5_values']\n",
    "\n",
    "\n",
    "merged_df = dfs[0]  # Start with the first DataFrame\n",
    "for df in dfs[1:]:  # Merge with the rest  #\n",
    "    merged_df = merged_df.merge(df, on=common_columns, how=\"outer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter where col1 contains 'messages', then drop duplicates based on col2\n",
    "df_final= merged_df[merged_df[\"path_1\"] == \"messages\"].drop_duplicates(subset=\"path_2\")\n",
    "\n",
    "\n",
    "df_final = df_final.replace('Missing', np.nan)\n",
    "\n",
    "\n",
    "print(df_final.isna().sum())\n",
    "\n",
    "# Save the final merged DataFrame\n",
    "df_final.to_csv(f\"{main_path}Youtube/Final/Merged_structures_YT.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
