{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Instagram Data Structures into a Schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The purpose of his jupyter notebook is to parse the collected data structures in earlier iterations of utilising the data donation tool into a schema_df which can be used to inform future iterations of the data donation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions\n",
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object', 'str', 'int', 'float', 'bool', 'dict', 'list']\n",
    "\n",
    "#Define the column names\n",
    "columns = ['col_path_1','col_path_2','col_path_3','col_path_4', 'col_path_5']\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(row, columns, data_types):\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "\n",
    "        #If the value stored in the column is found in the list 'data_types', \n",
    "        if row[column] in data_types:\n",
    "            # this value is placed in the column 'data_type'\n",
    "            row['data_type'] = row[column]\n",
    "            # and the original value is replaced with NA\n",
    "            row[column] = np.nan\n",
    "\n",
    "        #If the value is not found in the 'data_types' list, the original value is returned\n",
    "        else:\n",
    "            row[column]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_paths()\n",
    "The 'file_paths()' function splits up the paths to where the JSON file is stored in the folder and provides the name of the json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "    \n",
    "    df['value'] = df['value'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string_to_dict()\n",
    "As the JSON files are loaded as strings, they need to be converted to dictionaries to extract the values and be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(s):\n",
    "    # Check if the items needed for splitting are present. If not present it does not need to be splitted and the orginal value is returned\n",
    "    if ',' not in s and ':' not in s:\n",
    "        return s\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    result = {}\n",
    "\n",
    "    # Split the items by comma (split into key-value pairs)\n",
    "    items = s.split(',')\n",
    "\n",
    "    # For each item in the original dictionary\n",
    "    for item in items:\n",
    "        # Check if it contains a key-value pair, if not continue\n",
    "        if ':' not in item:\n",
    "            continue  \n",
    "\n",
    "        # Split the key-value pair into a variable 'key' and a variable 'value'\n",
    "        key, value = item.split(':', 1)  # use maxsplit=1 to avoid unpacking issue\n",
    "\n",
    "        # Try to strip any white spaces from the keys and values \n",
    "        try:\n",
    "            key = eval(key.strip())\n",
    "            value = eval(value.strip())\n",
    "        # If not possible, continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Save the converted and cleaned dictionary\n",
    "        result[key] = value\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_list()\n",
    "\n",
    "Due to lists in unexpected places in the JSON files and lengthy json paths, we need to identify the positions of lists to later select the correct get() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list if the data in the original structure is a list\n",
    "    \n",
    "def detect_list(x):\n",
    "    # If the data type of the value is list, 'LIST' is assigned in the '_LIST' columns (see'column_paths())\n",
    "    if isinstance(x, list):\n",
    "        return 'LIST'\n",
    "    # If the value is missing, 'MISSING' is assigned\n",
    "    elif pd.isna(x):\n",
    "        return 'MISSING'\n",
    "    # If the value is 'No data' (there is an empty place holder) 'MISSING' is assigned\n",
    "    elif x == 'No data':\n",
    "            return 'MISSING'\n",
    "    # if the value is not missing and is not a list, 'NO LIST' is assigned\n",
    "    else:\n",
    "        return 'NO LIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns_paths()\n",
    "Through this function, we map the json paths to each value by putting the keys in separate columns: colpath_{1,2,3,4,5} (see 'row_column_paths()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_paths(df, col_var, get_var, unlist_var, list_var):\n",
    "\n",
    "    # Convert to dict if it's a string\n",
    "    df[col_var] = df[col_var].apply(\n",
    "        lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "    \n",
    "        # Extract nested values using keys\n",
    "    \"\"\"\n",
    "    1. Selects the key obtained in the previous iteration stored in 'get_var'\n",
    "    2. The key put into a get() function which extracts values of a dictionary based on the keys\n",
    "    3. The get() function extracts the values from the (nested) dictionary stored in 'col_var'\n",
    "    4. If the get() function fails, None is returned\n",
    "    5. The steps above are only performed if the value stored in 'col_var' is a dictionary, otherwise None is returned\n",
    "    6. All steps above are executed for each row in the df\n",
    "    \"\"\"\n",
    "\n",
    "    df[get_var] = df.apply(\n",
    "    lambda row: list(row[col_var].keys()) if isinstance(row[col_var], dict) else row[col_var], axis=1)\n",
    "\n",
    "    \n",
    "    df = df.explode(get_var)\n",
    "\n",
    "    \n",
    "    # Extract nested values using keys\n",
    "    df[unlist_var] = df.apply(lambda row: row[col_var].get(row[get_var], None) if isinstance (row[col_var], dict) else None, axis=1)\n",
    "\n",
    "    # For each each column_path, check if the value contains a list\n",
    "    df[f'{list_var}_LIST'] = df[unlist_var].apply(detect_list)\n",
    "    \n",
    "     # Unlist data to avoid double nested lists \n",
    "    df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_column_paths()\n",
    "This function prepares the row paths and consequently executes the column_paths function to obtain the paths of keys (stored in individual columns) to reach the lowest value which is where the actual data is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_column_paths(df):\n",
    "    # Initialize the new columns\n",
    "    df['row_path'] = df['variable'] \n",
    "    df['col_path_1'] = ''  \n",
    "    df['col_path_2'] = ''  \n",
    "    df['col_path_3'] = ''\n",
    "    df['col_path_4'] = ''\n",
    "    df['col_path_5'] = '' \n",
    "\n",
    "    \n",
    "    # Initiate the list of columns needed to inform the function column_paths()\n",
    "    col_var = ['value', 'col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values']\n",
    "    get_var = ['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5']\n",
    "    unlist_var = ['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values']\n",
    "    list_var = ['col_path_2', 'col_path_3', 'col_path_4', 'col_path_5']\n",
    "\n",
    "    # Execute the colums_path() function\n",
    "    # zip is necessary due to the large number of variables needed in the column_paths function\n",
    "    for r, g, u, l in zip(col_var, get_var, unlist_var, list_var):\n",
    "        df = column_paths(df, r, g, u, l)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    " \n",
    "    # Reorder the columns in the df\n",
    "    df = df.loc[:, ['variable', 'value', 'row_path', \n",
    "                    'col_path_1', \n",
    "                     'col_path_2', 'col_path_2_LIST',\n",
    "                     'col_path_3',  'col_path_3_LIST',\n",
    "                     'col_path_4', 'col_path_4_LIST',\n",
    "                     'col_path_5', 'col_path_5_LIST',\n",
    "                     'data_type', 'col_path_1_values',\n",
    "                     'col_path_2_values','col_path_3_values',\n",
    "                     'col_path_4_values']]\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # fill na values with 'Missing'\n",
    "    df = df.fillna('Missing')\n",
    "\n",
    "\n",
    "    # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}TikTok/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    \"\"\"\n",
    "    data: The unprocessed data structure JSON that will be processed\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file (data structures)\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Strip top-level key\n",
    "    data = list(data.values())[0]\n",
    "\n",
    "\n",
    "    # Flatten JSON (handling nested structures)\n",
    "    df = pd.json_normalize(data, max_level=0)\n",
    "\n",
    "    # Extract column names\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "\n",
    "    # From wide to long df\n",
    "    df = pd.melt(df, value_vars= cols)\n",
    "\n",
    "    # Execute the 'file_paths()' function and store the result in df\n",
    "    df = file_paths(df)\n",
    "\n",
    "    # Execute the 'row_column_paths()' function and store the result in df\n",
    "    df = row_column_paths(df)\n",
    "    \n",
    "    # Execute the 'process_col_path()' function and store the result in df\n",
    "    df = df.apply(lambda row: process_col_path(row, columns, data_types), axis=1)\n",
    "\n",
    "    # Execute the 'clean_and_store()' function and store the result in df\n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/TikTok/Input_test\n"
     ]
    }
   ],
   "source": [
    "input_directory = Path(f'{main_path}TikTok/Input_test')  \n",
    "print(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85756/766349402.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df = merged_df.replace('Missing', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}TikTok/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Replace 'Missing' with NaN\n",
    "merged_df = merged_df.replace('Missing', np.nan)\n",
    "\n",
    "\n",
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}TikTok/Final/Merged_structures_TT.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
