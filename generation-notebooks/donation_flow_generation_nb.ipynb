{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97e3def",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 1. Python code to dynamically create the Instagram script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "# Load schema       - Alt Path (/mnt/c/Users/arodilla/Downloads/Merged_structures_IG.csv)\n",
    "schema_df = pd.read_csv('/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_IG.csv')\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "schema_df = schema_df.dropna(subset=[\"variable\", \"value\"])\n",
    "\n",
    "# Helper function to flatten the schema and map to simplified column names\n",
    "def flatten_schema(schema: dict, parent_key: str = '') -> dict:\n",
    "    \"\"\"Recursively flattens nested schema dicts to dot notation paths and returns user-friendly column names.\"\"\"\n",
    "    items = {}\n",
    "    for k, v in schema.items():\n",
    "        new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_schema(v, new_key))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def generate_df_function_by_json(json_name: str, group: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates a parsing function per json_name, using 'row_path' for row selection\n",
    "    and up to 5 levels of nested 'col_path_*' fields for column mapping.\n",
    "    \"\"\"\n",
    "    json_name_no_ext = json_name.replace(\".json\", \"\")\n",
    "    row_path = group[\"row_path\"].iloc[0]  # updated to use new column\n",
    "\n",
    "    # Clean function name\n",
    "    func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name_no_ext.lower()) + \"_df\"\n",
    "\n",
    "    col_paths = {}\n",
    "    col_path_fields = [\"col_path_1\", \"col_path_2\", \"col_path_3\", \"col_path_4\", \"col_path_5\"]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        # Extract all non-null parts of the path\n",
    "        path_parts = [str(row[col]) for col in col_path_fields if pd.notna(row.get(col))]\n",
    "        if not path_parts:\n",
    "            continue\n",
    "\n",
    "        # The column name is the last part\n",
    "        col_name = path_parts[-1]\n",
    "        col_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", col_name.strip().replace(\" \", \"_\"))\n",
    "\n",
    "        json_path = \".\".join(path_parts)\n",
    "        col_paths[col_name] = [json_path]\n",
    "\n",
    "    # Start function definition\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(file_input: list[str]) -> pd.DataFrame:\")\n",
    "    lines.append(f'    data = read_json(file_input, [\"*/{json_name_no_ext}.json\"])')\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"    df = parse_json(data,\")\n",
    "    lines.append(f'        row_path=[\"$.{row_path}\"],')\n",
    "    lines.append(\"        col_paths=dict(\")\n",
    "\n",
    "    for col_name, path in col_paths.items():\n",
    "        lines.append(f'        {col_name} = {path},')\n",
    "\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    )\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append('    if \"time\" in df.columns:')\n",
    "    lines.append('        df[\"date\"] = pd.to_datetime(df[\"time\"], unit=\"s\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")')\n",
    "    lines.append('        df = df.sort_values(\"date\")')\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append(\"    return df\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_donation_flow_function_explicit(schema_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates an explicit version of create_donation_flow that includes hardcoded try/except\n",
    "    blocks per known extractor function.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"def create_donation_flow(file_input: list[str]):\")\n",
    "    lines.append('    \"\"\"')\n",
    "    lines.append(\"    Creates a donation flow for Instagram data, explicitly trying each extractor function.\")\n",
    "    lines.append(\"    Only creates tables for data that's available in the provided files.\")\n",
    "    lines.append('    \"\"\"')\n",
    "    lines.append(\"    tables = []\")\n",
    "    lines.append(\"    #print(file_input)\\n\")\n",
    "\n",
    "    grouped = schema_df.groupby(\"json_name\")\n",
    "\n",
    "    for json_name, group in grouped:\n",
    "        #name_base = json_name.replace(\".json\", \"\").replace(\"'\",\"_\").replace(\"-\",\"_\").lower()\n",
    "        name_base = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name.replace(\".json\", \"\").lower())\n",
    "        func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", name_base) + \"_df\"\n",
    "        table_name = name_base\n",
    "\n",
    "        # Basic fallback titles (could be improved by a proper map)\n",
    "        #raw_title = group[\"value\"].iloc[0] if pd.notna(group[\"value\"].iloc[0]) else table_name.replace(\"_\", \" \").title()\n",
    "        #escaped_title = raw_title.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "        escaped_title = table_name\n",
    "        english_title = escaped_title\n",
    "        dutch_title = escaped_title  # or translate if needed\n",
    "\n",
    "        lines.append(f\"    # {english_title}\")\n",
    "        lines.append(\"    try:\")\n",
    "        lines.append(f\"        {table_name}_table = donation_table(\")\n",
    "        lines.append(f'            name=\"{table_name}\",')\n",
    "        lines.append(f\"            df={func_name}(file_input),\")\n",
    "        lines.append(f'            title={{\\\"en\\\": \\\"{english_title}\\\", \\\"nl\\\": \\\"{dutch_title}\\\"}},')\n",
    "        lines.append(\"        )\")\n",
    "        lines.append(f\"        tables.append({table_name}_table)\")\n",
    "        lines.append(\"    except Exception as e:\")\n",
    "        lines.append(f'        #print(f\\\"Skipping {table_name}: {{e}}\\\")')\n",
    "        lines.append(\"        pass\\n\")\n",
    "\n",
    "    lines.append(\"    # Only create the donation flow if we have at least one table\")\n",
    "    lines.append(\"    if tables:\")\n",
    "    lines.append(\"        return donation_flow(\")\n",
    "    lines.append('            id=\\\"instagram\\\",')\n",
    "    lines.append(\"            tables=tables\")\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    else:\")\n",
    "    lines.append('        #print(\\\"No tables could be generated from the provided files\\\")')\n",
    "    lines.append(\"        return None\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Group by json_name (1 table per JSON file)\n",
    "generated_functions = [\n",
    "    generate_df_function_by_json(json_name, group)\n",
    "    for json_name, group in schema_df.groupby(\"json_name\")\n",
    "]\n",
    "\n",
    "# Function to create donation flow (remains the same)\n",
    "def create_donation_flow(file_input: list[str]):\n",
    "    \"\"\"\n",
    "    Creates a donation flow using pre-defined extractors applied to file_input ZIP.\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "\n",
    "    # List of all available extractor functions dynamically\n",
    "    extraction_functions = {}\n",
    "\n",
    "    for row in schema_df.iterrows():\n",
    "        filename = row[1][\"variable\"].split(\"/\")[-1]\n",
    "        func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", filename.replace(\".json\", \"\").lower()) + \"_df\"\n",
    "        extraction_functions[filename] = globals().get(func_name)\n",
    "\n",
    "    # Run the extractors\n",
    "    for filename, extractor_func in extraction_functions.items():\n",
    "        try:\n",
    "            df = pd.DataFrame(extractor_func(file_input))\n",
    "            if not df.empty:\n",
    "                table_name = filename.replace(\".json\", \"\").capitalize()\n",
    "                tables.append(\n",
    "                    donation_table(\n",
    "                        name=table_name,\n",
    "                        df=df,\n",
    "                        title={\"en\": table_name}\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running {extractor_func.__name__}: {e}\")\n",
    "\n",
    "    return donation_flow(\n",
    "        id=\"Instagram\",\n",
    "        tables=tables\n",
    "    )\n",
    "\n",
    "# Save the generated functions to a file\n",
    "all_code = \"\\n\\n\".join(generated_functions)\n",
    "# Get the source code of the create_donation_flow function as a string\n",
    "donation_flow_function_str = generate_donation_flow_function_explicit(schema_df)\n",
    "\n",
    "# Save the generated functions to a file\n",
    "all_code = \"\\n\\n\".join(generated_functions)\n",
    "with open(\"src/framework/processing/py/port/helpers/instagram_generated_extractors.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated Instagram extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "    f.write(\"from port.helpers.readers import read_json\\n\")\n",
    "    f.write(\"from port.helpers.parsers import parse_json\\n\\n\")\n",
    "    f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "    f.write(all_code)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_function_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee640c",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 2. Python code to dynamically create the Facebook script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "import keyword \n",
    "\n",
    "# Load schema       - Alt Path (/mnt/c/Users/arodilla/Downloads/Merged_structures_IG.csv)\n",
    "schema_df = pd.read_csv('/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_FB.csv')\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "schema_df = schema_df.dropna(subset=[\"variable\", \"value\"])\n",
    "\n",
    "# Helper function to flatten the schema and map to simplified column names\n",
    "def flatten_schema(schema: dict, parent_key: str = '') -> dict:\n",
    "    \"\"\"Recursively flattens nested schema dicts to dot notation paths and returns user-friendly column names.\"\"\"\n",
    "    items = {}\n",
    "    for k, v in schema.items():\n",
    "        new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_schema(v, new_key))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def generate_df_function_by_json(json_name: str, group: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates a parsing function per json_name, using 'row_path' for row selection\n",
    "    and up to 5 levels of nested 'col_path_*' fields for column mapping.\n",
    "    \"\"\"\n",
    "    json_name_no_ext = json_name.replace(\".json\", \"\")\n",
    "    row_path = group[\"row_path\"].iloc[0]  # updated to use new column\n",
    "\n",
    "    # Clean function name\n",
    "    func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name_no_ext.lower()) + \"_df\"\n",
    "\n",
    "    col_paths = {}\n",
    "    col_path_fields = [\"col_path_1\", \"col_path_2\", \"col_path_3\", \"col_path_4\", \"col_path_5\"]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        # Extract all non-null parts of the path\n",
    "        path_parts = [str(row[col]) for col in col_path_fields if pd.notna(row.get(col))]\n",
    "        if not path_parts:\n",
    "            continue\n",
    "\n",
    "        # The column name is the last part\n",
    "        col_name = path_parts[-1]\n",
    "        col_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", col_name.strip().replace(\" \", \"_\"))\n",
    "\n",
    "        # If it's a Python keyword, add underscore\n",
    "        if keyword.iskeyword(col_name):\n",
    "            col_name += \"_\"\n",
    "\n",
    "        json_path = \".\".join(path_parts)\n",
    "        col_paths[col_name] = [json_path]\n",
    "\n",
    "    # Start function definition\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(file_input: list[str]) -> pd.DataFrame:\")\n",
    "    lines.append(f'    data = read_json(file_input, [\"*/{json_name_no_ext}.json\"])')\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"    df = parse_json(data,\")\n",
    "    lines.append(f'        row_path=[\"$.{row_path}\"],')\n",
    "    lines.append(\"        col_paths=dict(\")\n",
    "\n",
    "    for col_name, path in col_paths.items():\n",
    "        lines.append(f'        {col_name} = {path},')\n",
    "\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    )\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append('    if \"time\" in df.columns:')\n",
    "    lines.append('        df[\"date\"] = pd.to_datetime(df[\"time\"], unit=\"s\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")')\n",
    "    lines.append('        df = df.sort_values(\"date\")')\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append(\"    return df\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_donation_flow_function_explicit(schema_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates an explicit version of create_donation_flow that includes hardcoded try/except\n",
    "    blocks per known extractor function.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"def create_donation_flow(file_input: list[str]):\")\n",
    "    lines.append('    \"\"\"')\n",
    "    lines.append(\"    Creates a donation flow for Facebook data, explicitly trying each extractor function.\")\n",
    "    lines.append(\"    Only creates tables for data that's available in the provided files.\")\n",
    "    lines.append('    \"\"\"')\n",
    "    lines.append(\"    tables = []\")\n",
    "    lines.append(\"    #print(file_input)\\n\")\n",
    "\n",
    "    grouped = schema_df.groupby(\"json_name\")\n",
    "\n",
    "    for json_name, group in grouped:\n",
    "        #name_base = json_name.replace(\".json\", \"\").replace(\"'\",\"_\").replace(\"-\",\"_\").lower()\n",
    "        name_base = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name.replace(\".json\", \"\").lower())\n",
    "        func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", name_base) + \"_df\"\n",
    "        table_name = name_base\n",
    "\n",
    "        # Basic fallback titles (could be improved by a proper map)\n",
    "        #raw_title = group[\"value\"].iloc[0] if pd.notna(group[\"value\"].iloc[0]) else table_name.replace(\"_\", \" \").title()\n",
    "        #escaped_title = raw_title.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "        escaped_title = table_name\n",
    "        english_title = escaped_title\n",
    "        dutch_title = escaped_title  # or translate if needed\n",
    "\n",
    "        lines.append(f\"    # {english_title}\")\n",
    "        lines.append(\"    try:\")\n",
    "        lines.append(f\"        {table_name}_table = donation_table(\")\n",
    "        lines.append(f'            name=\"{table_name}\",')\n",
    "        lines.append(f\"            df={func_name}(file_input),\")\n",
    "        lines.append(f'            title={{\\\"en\\\": \\\"{english_title}\\\", \\\"nl\\\": \\\"{dutch_title}\\\"}},')\n",
    "        lines.append(\"        )\")\n",
    "        lines.append(f\"        tables.append({table_name}_table)\")\n",
    "        lines.append(\"    except Exception as e:\")\n",
    "        lines.append(f'        #print(f\\\"Skipping {table_name}: {{e}}\\\")')\n",
    "        lines.append(\"        pass\\n\")\n",
    "\n",
    "    lines.append(\"    # Only create the donation flow if we have at least one table\")\n",
    "    lines.append(\"    if tables:\")\n",
    "    lines.append(\"        return donation_flow(\")\n",
    "    lines.append('            id=\\\"facebook\\\",')\n",
    "    lines.append(\"            tables=tables\")\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    else:\")\n",
    "    lines.append('        #print(\\\"No tables could be generated from the provided files\\\")')\n",
    "    lines.append(\"        return None\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Group by json_name (1 table per JSON file)\n",
    "generated_functions = [\n",
    "    generate_df_function_by_json(json_name, group)\n",
    "    for json_name, group in schema_df.groupby(\"json_name\")\n",
    "]\n",
    "\n",
    "# Function to create donation flow (remains the same)\n",
    "def create_donation_flow(file_input: list[str]):\n",
    "    \"\"\"\n",
    "    Creates a donation flow using pre-defined extractors applied to file_input ZIP.\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "\n",
    "    # List of all available extractor functions dynamically\n",
    "    extraction_functions = {}\n",
    "\n",
    "    for row in schema_df.iterrows():\n",
    "        filename = row[1][\"variable\"].split(\"/\")[-1]\n",
    "        func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", filename.replace(\".json\", \"\").lower()) + \"_df\"\n",
    "        extraction_functions[filename] = globals().get(func_name)\n",
    "\n",
    "    # Run the extractors\n",
    "    for filename, extractor_func in extraction_functions.items():\n",
    "        try:\n",
    "            df = pd.DataFrame(extractor_func(file_input))\n",
    "            if not df.empty:\n",
    "                table_name = filename.replace(\".json\", \"\").capitalize()\n",
    "                tables.append(\n",
    "                    donation_table(\n",
    "                        name=table_name,\n",
    "                        df=df,\n",
    "                        title={\"en\": table_name}\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running {extractor_func.__name__}: {e}\")\n",
    "\n",
    "    return donation_flow(\n",
    "        id=\"Facebook\",\n",
    "        tables=tables\n",
    "    )\n",
    "\n",
    "# Save the generated functions to a file\n",
    "all_code = \"\\n\\n\".join(generated_functions)\n",
    "# Get the source code of the create_donation_flow function as a string\n",
    "donation_flow_function_str = generate_donation_flow_function_explicit(schema_df)\n",
    "\n",
    "# Save the generated functions to a file\n",
    "all_code = \"\\n\\n\".join(generated_functions)\n",
    "with open(\"src/framework/processing/py/port/helpers/facebook_generated_extractors.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated Facebook extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "    f.write(\"from port.helpers.readers import read_json\\n\")\n",
    "    f.write(\"from port.helpers.parsers import parse_json\\n\\n\")\n",
    "    f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "    f.write(all_code)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_function_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f81569",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 3. Python code to dynamically create the TikTok script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f17e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extractors written to /home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/tiktok_generated_extractors.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import inspect\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------\n",
    "# Utility Functions\n",
    "# -----------------------\n",
    "def get_in(d: dict, *keys):\n",
    "    for key in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(key)\n",
    "        else:\n",
    "            return None\n",
    "    return d\n",
    "\n",
    "def get_list(d: dict, *keys):\n",
    "    val = get_in(d, *keys)\n",
    "    return val if isinstance(val, list) else []\n",
    "\n",
    "def get_dict(d: dict, *keys):\n",
    "    val = get_in(d, *keys)\n",
    "    return val if isinstance(val, dict) else {}\n",
    "\n",
    "def snake_case(name: str) -> str:\n",
    "    return name.lower().replace(\"-\", \"_\").replace(\".json\", \"\").replace(\".js\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "def get_field_name(row):\n",
    "    return next((row.get(f\"col_{i}\") for i in range(7, 2, -1) if pd.notna(row.get(f\"col_{i}\"))), None)\n",
    "\n",
    "\n",
    "def extract_path(row):\n",
    "    path = []\n",
    "    for col in [f\"col_{i}\" for i in range(1, 8)]:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val):\n",
    "            path.append(str(val).strip())\n",
    "    return path\n",
    "\n",
    "def is_list_index(val):\n",
    "    return str(val).isdigit()\n",
    "\n",
    "# -----------------------\n",
    "# Load and Clean Schema\n",
    "# -----------------------\n",
    "schema_path = '/mnt/c/Users/arodilla/Downloads/Structure output test - test.csv.csv'\n",
    "schema_df = pd.read_csv(schema_path)\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "\n",
    "# -----------------------\n",
    "# Generate Extractor Function\n",
    "# -----------------------\n",
    "def generate_df_function(file_folder_name: str, group: pd.DataFrame) -> str:\n",
    "    original_root_key = file_folder_name\n",
    "    func_name = f\"{snake_case(file_folder_name)}_df\"\n",
    "\n",
    "    lines = [\n",
    "        f\"def {func_name}(file_input: List[str]) -> pd.DataFrame:\",\n",
    "        \"    try:\",\n",
    "        \"        with open(file_input[0], 'r', encoding='utf-8') as f:\",\n",
    "        \"            data = json.load(f)\",\n",
    "        f\"        root_data = get_in(data, '{original_root_key}')\",\n",
    "        \"        if not root_data:\",\n",
    "        f\"            print(f'⚠️ No data found at path: {original_root_key}')\",\n",
    "        \"            return pd.DataFrame()\",\n",
    "        \"\",\n",
    "        \"        base_row = {}\",\n",
    "    ]\n",
    "\n",
    "    static_fields = []\n",
    "    list_blocks = {}  # key = tuple(list_path), value = set(fields)\n",
    "    seen_static = set()\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        path = extract_path(row)\n",
    "        if path[0] == original_root_key:\n",
    "            path = path[1:]\n",
    "\n",
    "        for i, key in enumerate(path):\n",
    "            if is_list_index(key):\n",
    "                list_path = tuple(path[:i])  # use tuple for dict key\n",
    "                field = path[i + 1] if i + 1 < len(path) else None\n",
    "                if field:\n",
    "                    list_blocks.setdefault(list_path, set()).add(field)\n",
    "                break\n",
    "        else:\n",
    "            field = path[-1]\n",
    "            if field not in seen_static:\n",
    "                static_fields.append(path)\n",
    "                seen_static.add(field)\n",
    "\n",
    "    # Static field extraction\n",
    "    for path in static_fields:\n",
    "        field = path[-1]\n",
    "        path_str = \"', '\".join(path)\n",
    "        lines.append(f\"        base_row['{field}'] = get_in(root_data, '{path_str}')\")\n",
    "\n",
    "    # If no list blocks, return single row\n",
    "    if not list_blocks:\n",
    "        lines.append(\"        return pd.DataFrame([base_row])\")\n",
    "    else:\n",
    "        lines.append(\"        all_records = []\")\n",
    "\n",
    "        for list_path, fields in list_blocks.items():\n",
    "            path_str = \"', '\".join(list_path)\n",
    "            lines += [\n",
    "                f\"        items = get_list(root_data, '{path_str}')\",\n",
    "                \"        for item in items:\",\n",
    "                \"            row = base_row.copy()\"\n",
    "            ]\n",
    "            for field in sorted(fields):\n",
    "                lines.append(f\"            row['{field}'] = item.get('{field}', '.*?')\")\n",
    "            lines.append(\"            all_records.append(row)\")\n",
    "\n",
    "        lines.append(\"        return pd.DataFrame(all_records)\")\n",
    "\n",
    "    lines += [\n",
    "        \"    except Exception as e:\",\n",
    "        f\"        print(f'❌ Error in {func_name}:', e)\",\n",
    "        \"        return pd.DataFrame()\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Generate Donation Flow Function\n",
    "# -----------------------\n",
    "def generate_donation_flow(schema_df: pd.DataFrame) -> str:\n",
    "    lines = [\n",
    "        \"def create_donation_flow(file_input: List[str]):\",\n",
    "        '    \"\"\"Create donation flow from TikTok JSON.\"\"\"',\n",
    "        \"    tables = []\",\n",
    "        \"\"\n",
    "    ]\n",
    "    for file_name, group in schema_df.groupby(\"col_1\"):\n",
    "        root_key = snake_case(file_name)\n",
    "        func_name = f\"{root_key}_df\"\n",
    "        lines += [\n",
    "            f\"    try:\",\n",
    "            f\"        df = {func_name}(file_input)\",\n",
    "            f\"        if not df.empty:\",\n",
    "            f\"            tables.append(\",\n",
    "            f\"                donation_table(name='{root_key}', df=df, title={{'en': '{root_key}'}})\",\n",
    "            f\"            )\",\n",
    "            f\"    except Exception as e:\",\n",
    "            f\"        print(f'Error in {func_name}:', e)\",\n",
    "            \"\"\n",
    "        ]\n",
    "    lines += [\n",
    "        \"    if tables:\",\n",
    "        \"        return donation_flow(id='tiktok', tables=tables)\",\n",
    "        \"    else:\",\n",
    "        \"        return None\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Generate & Write Output\n",
    "# -----------------------\n",
    "generated_functions = [\n",
    "    generate_df_function(file_name, group)\n",
    "    for file_name, group in schema_df.groupby(\"col_1\")\n",
    "]\n",
    "\n",
    "donation_flow_code = generate_donation_flow(schema_df)\n",
    "\n",
    "output_path = Path(\"/home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/tiktok_generated_extractors.py\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated TikTok extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import json\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "    f.write(\"from typing import List\\n\\n\")\n",
    "    f.write(inspect.getsource(get_in))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(inspect.getsource(get_list))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(inspect.getsource(get_dict))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(generated_functions))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_code)\n",
    "\n",
    "print(f\"✅ Extractors written to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9bfd4",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 4. Python code to dynamically create the Twitter script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58953708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import zipfile\n",
    "import logging\n",
    "import inspect\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Load Schema ---\n",
    "schema_df = pd.read_csv('/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_X.csv')\n",
    "schema_df = schema_df.dropna(subset=[\"key\"])\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "\n",
    "# --- Helper to Read .js files ---\n",
    "def read_js(file_input: list[str], target_files: list[str]) -> list:\n",
    "    \"\"\"Extracts JSON content from matching .js files inside the ZIP.\"\"\"\n",
    "    extracted_data = []\n",
    "\n",
    "    for zip_path in file_input:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            for target_file in target_files:\n",
    "                js_files = [f for f in z.namelist() if target_file in f]\n",
    "                if js_files:\n",
    "                    with z.open(js_files[0]) as raw_file:\n",
    "                        with io.TextIOWrapper(raw_file, encoding=\"utf8\") as text_file:\n",
    "                            lines = text_file.readlines()\n",
    "                        lines[0] = re.sub(r\"^.*? = \", \"\", lines[0])  # remove variable assignment\n",
    "                        try:\n",
    "                            data = json.loads(\"\".join(lines))\n",
    "                            extracted_data.extend(data)  # assuming a list inside\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            logger.error(f\"Error decoding {target_file} in {zip_path}: {e}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# --- Generate Function for Each .js File ---\n",
    "def generate_df_function(file_folder_name: str, group: pd.DataFrame) -> str:\n",
    "    table_base = file_folder_name.replace('.js', '').replace('-', '_')  # First level key from the file name\n",
    "    func_name = f\"{table_base}_df\"\n",
    "    file_clean = file_folder_name\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(file_input: list[str]) -> pd.DataFrame:\")\n",
    "    lines.append(f\"    data = read_js(file_input, ['{file_clean}'])\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"    records = []\")\n",
    "    lines.append(\"    for item in data:\")\n",
    "    lines.append(\"        record = {}\")\n",
    "\n",
    "    # For each key in the schema, the first level is the file name (table_base)\n",
    "    for key in group[\"key\"].dropna():\n",
    "        # Remove any non-word characters for the key, ensure it starts with a letter or underscore\n",
    "        safe_key = re.sub(r\"\\W|^(?=\\d)\", \"_\", str(key))\n",
    "        \n",
    "        # The key is always nested with the first level being the file name\n",
    "        nested_key = key  # Since the key will always be inside the first-level key (file name)\n",
    "        \n",
    "        lines.append(f\"        {table_base} = item.get('{table_base}', {{}})\")  # Extract the base part of the key (first level)\n",
    "        lines.append(f\"        record['{safe_key}'] = {table_base}.get('{nested_key}', '.*?')\")  # Access the nested key\n",
    "\n",
    "    lines.append(\"        records.append(record)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"    df = pd.DataFrame(records)\")\n",
    "    lines.append(\"    return df\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Generate Donation Flow ---\n",
    "def generate_donation_flow(schema_df: pd.DataFrame) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"def create_donation_flow(file_input: list[str]):\")\n",
    "    lines.append('    \"\"\"Create a donation flow for Twitter data.\"\"\"')\n",
    "    lines.append(\"    tables = []\\n\")\n",
    "\n",
    "    for file_folder_name, group in schema_df.groupby(\"file_folder_name\"):\n",
    "        table_base = file_folder_name.replace('.js', '').replace('-', '_')\n",
    "        func_name = f\"{table_base}_df\"\n",
    "\n",
    "        lines.append(f\"    try:\")\n",
    "        lines.append(f\"        df = {func_name}(file_input)\")\n",
    "        lines.append(f\"        if not df.empty:\")\n",
    "        lines.append(f\"            tables.append(\")\n",
    "        lines.append(f\"                donation_table(name='{table_base}', df=df, title={{'en': '{table_base}'}})\")\n",
    "        lines.append(\"            )\")\n",
    "        lines.append(\"    except Exception as e:\")\n",
    "        lines.append(f\"        logger.error(f'Skipping {func_name}: {{e}}')\\n\")\n",
    "\n",
    "    lines.append(\"    if tables:\")\n",
    "    lines.append(\"        return donation_flow(id='twitter', tables=tables)\")\n",
    "    lines.append(\"    else:\")\n",
    "    lines.append(\"        return None\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Actually Generate Code ---\n",
    "\n",
    "# Generate extractor functions\n",
    "generated_functions = [\n",
    "    generate_df_function(file_folder_name, group)\n",
    "    for file_folder_name, group in schema_df.groupby(\"file_folder_name\")\n",
    "]\n",
    "\n",
    "# Generate donation flow function\n",
    "donation_flow_function_str = generate_donation_flow(schema_df)\n",
    "\n",
    "# Save everything into a file\n",
    "with open(\"/home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/twitter_generated_extractors.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated Twitter extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import json\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"import io\\n\")\n",
    "    f.write(\"import zipfile\\n\")\n",
    "    f.write(\"import re\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\\n\")\n",
    "    f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "    f.write(inspect.getsource(read_js))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(generated_functions))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_function_str)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
